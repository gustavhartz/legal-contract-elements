{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ab92337",
   "metadata": {},
   "source": [
    "## NLP Preprocessing of CUDA dataset\n",
    "\n",
    "\n",
    "Almost excactly the same process as when dealing with the drqa dataset excpet we don't utilize the normalization anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9d0ff3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e8a378",
   "metadata": {},
   "source": [
    "We create a dataframe as in the drqa dataset. The filters applied will be to filter cases where there doesn't exists a signle answer or where the dataset has split the answer into multiple sections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "7049b489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = []\n",
    "i=0\n",
    "with open(\"../data/raw/CUAD_v1/CUAD_v1.json\", encoding=\"utf-8\") as f:\n",
    "    cuad = json.load(f)\n",
    "    # Contract\n",
    "    for example in cuad[\"data\"]:\n",
    "        title = example.get(\"title\", \"\").strip()\n",
    "        # Paragraph in contract\n",
    "        # We only look at the first one\n",
    "        for paragraph in example[\"paragraphs\"]:\n",
    "            context = paragraph[\"context\"].strip()\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                if qa.get(\"is_impossible\"):\n",
    "                    continue\n",
    "                question = qa[\"question\"].strip()\n",
    "                id_ = qa[\"id\"]\n",
    "\n",
    "                answer_starts = [answer[\"answer_start\"] for answer in qa[\"answers\"]]\n",
    "                answers = [answer[\"text\"].strip() for answer in qa[\"answers\"]]\n",
    "                answer_end = [context.find(x)+len(x) for x in answers if context.find(x)]\n",
    "                if len(answers)>1:\n",
    "                    continue\n",
    "                if answer_end and len(context[min(answer_starts):max(answer_end)])<2000:\n",
    "                    ctx_offset = max(1,min(answer_starts)-200)\n",
    "                    data.append((id_, question, answers[0], [answer_starts[0]-ctx_offset, answer_end[0]-ctx_offset], context[ctx_offset:min(len(context),max(answer_end)+500)], ctx_offset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9a28326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.DataFrame(data, columns=[\"id_\", \"question\", \"answer\", \"label\", \"context\", \"ctx_offset\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "4c8c5bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['question'].isin(['Highlight the parts (if any) of this contract related to \"Document Name\" that should be reviewed by a lawyer. Details: The name of the contract',\n",
    "                    'Highlight the parts (if any) of this contract related to \"Agreement Date\" that should be reviewed by a lawyer. Details: The date of the contract',\n",
    "                    \"\"\"'Highlight the parts (if any) of this contract related to \"Expiration Date\" that should be reviewed by a lawyer. Details: On what date will the contract\\'s initial term expire?'\"\"\",\n",
    "                    'Highlight the parts (if any) of this contract related to \"Renewal Term\" that should be reviewed by a lawyer. Details: What is the renewal term after the initial term expires? This includes automatic extensions and unilateral extensions with prior notice.',\n",
    "                    ])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "5eb2be43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>label</th>\n",
       "      <th>context</th>\n",
       "      <th>ctx_offset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LIMEENERGYCO_09_09_1999-EX-10-DISTRIBUTOR AGRE...</td>\n",
       "      <td>Highlight the parts (if any) of this contract ...</td>\n",
       "      <td>DISTRIBUTOR AGREEMENT</td>\n",
       "      <td>[43, 64]</td>\n",
       "      <td>XHIBIT 10.6\\n\\n                              D...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LIMEENERGYCO_09_09_1999-EX-10-DISTRIBUTOR AGRE...</td>\n",
       "      <td>Highlight the parts (if any) of this contract ...</td>\n",
       "      <td>7th day of September, 1999.</td>\n",
       "      <td>[200, 227]</td>\n",
       "      <td>NT\\n\\n         THIS  DISTRIBUTOR  AGREEMENT (t...</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LIMEENERGYCO_09_09_1999-EX-10-DISTRIBUTOR AGRE...</td>\n",
       "      <td>Highlight the parts (if any) of this contract ...</td>\n",
       "      <td>If Distributor                            comp...</td>\n",
       "      <td>[200, 538]</td>\n",
       "      <td>ears (the \"Term\")  which shall  commence on th...</td>\n",
       "      <td>5343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WHITESMOKE,INC_11_08_2011-EX-10.26-PROMOTION A...</td>\n",
       "      <td>Highlight the parts (if any) of this contract ...</td>\n",
       "      <td>Promotion and Distribution Agreement</td>\n",
       "      <td>[200, 236]</td>\n",
       "      <td>CH PORTION,  WHICH HAS BEEN OMITTED HEREIN AND...</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LohaCompanyltd_20191209_F-1_EX-10.16_11917878_...</td>\n",
       "      <td>Highlight the parts (if any) of this contract ...</td>\n",
       "      <td>SUPPLY CONTRACT</td>\n",
       "      <td>[13, 28]</td>\n",
       "      <td>xhibit 10.16 SUPPLY CONTRACT Contract No: Date...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id_  \\\n",
       "0   LIMEENERGYCO_09_09_1999-EX-10-DISTRIBUTOR AGRE...   \n",
       "1   LIMEENERGYCO_09_09_1999-EX-10-DISTRIBUTOR AGRE...   \n",
       "3   LIMEENERGYCO_09_09_1999-EX-10-DISTRIBUTOR AGRE...   \n",
       "8   WHITESMOKE,INC_11_08_2011-EX-10.26-PROMOTION A...   \n",
       "14  LohaCompanyltd_20191209_F-1_EX-10.16_11917878_...   \n",
       "\n",
       "                                             question  \\\n",
       "0   Highlight the parts (if any) of this contract ...   \n",
       "1   Highlight the parts (if any) of this contract ...   \n",
       "3   Highlight the parts (if any) of this contract ...   \n",
       "8   Highlight the parts (if any) of this contract ...   \n",
       "14  Highlight the parts (if any) of this contract ...   \n",
       "\n",
       "                                               answer       label  \\\n",
       "0                               DISTRIBUTOR AGREEMENT    [43, 64]   \n",
       "1                         7th day of September, 1999.  [200, 227]   \n",
       "3   If Distributor                            comp...  [200, 538]   \n",
       "8                Promotion and Distribution Agreement  [200, 236]   \n",
       "14                                    SUPPLY CONTRACT    [13, 28]   \n",
       "\n",
       "                                              context  ctx_offset  \n",
       "0   XHIBIT 10.6\\n\\n                              D...           1  \n",
       "1   NT\\n\\n         THIS  DISTRIBUTOR  AGREEMENT (t...          63  \n",
       "3   ears (the \"Term\")  which shall  commence on th...        5343  \n",
       "8   CH PORTION,  WHICH HAS BEEN OMITTED HEREIN AND...         107  \n",
       "14  xhibit 10.16 SUPPLY CONTRACT Contract No: Date...           1  "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f80590ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_text_for_vocab(dfs: list):\n",
    "    '''\n",
    "    Gathers text from contexts and questions to build a vocabulary.\n",
    "\n",
    "    :param dfs: list of dataframes of SQUAD dataset.\n",
    "    :returns: list of contexts and questions\n",
    "    '''\n",
    "\n",
    "    text = []\n",
    "    total = 0\n",
    "    for df in dfs:\n",
    "        unique_contexts = list(df.context.unique())\n",
    "        unique_questions = list(df.question.unique())\n",
    "        total += df.context.nunique() + df.question.nunique()\n",
    "        text.extend(unique_contexts + unique_questions)\n",
    "\n",
    "    assert len(text) == total\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "00c85480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def build_word_vocab(vocab_text):\n",
    "    '''\n",
    "    Builds a word-level vocabulary from the given text.\n",
    "\n",
    "    :param list vocab_text: list of contexts and questions\n",
    "    :returns \n",
    "        dict word2idx: word to index mapping of words\n",
    "        dict idx2word: integer to word mapping\n",
    "        list word_vocab: list of words sorted by frequency\n",
    "    '''\n",
    "\n",
    "    words = []\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    for sent in vocab_text:\n",
    "        for word in nlp(sent, disable=['parser', 'tagger', 'ner']):\n",
    "            words.append(word.text)\n",
    "\n",
    "    word_counter = Counter(words)\n",
    "    word_vocab = sorted(word_counter, key=word_counter.get, reverse=True)\n",
    "    print(f\"raw-vocab: {len(word_vocab)}\")\n",
    "    word_vocab.insert(0, '<unk>')\n",
    "    word_vocab.insert(1, '<pad>')\n",
    "    print(f\"vocab-length: {len(word_vocab)}\")\n",
    "    word2idx = {word: idx for idx, word in enumerate(word_vocab)}\n",
    "    print(f\"word2idx-length: {len(word2idx)}\")\n",
    "    idx2word = {v: k for k, v in word2idx.items()}\n",
    "\n",
    "    return word2idx, idx2word, word_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "418b4e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_text = gather_text_for_vocab([df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "51650789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw-vocab: 9816\n",
      "vocab-length: 9818\n",
      "word2idx-length: 9818\n"
     ]
    }
   ],
   "source": [
    "word2idx, idx2word, word_vocab = build_word_vocab(vocab_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "2d6e3ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_answer(row, idx2word):\n",
    "    '''\n",
    "    Takes in a row of the dataframe or one training example and\n",
    "    returns a tuple of start and end positions of answer by calculating \n",
    "    spans.\n",
    "    '''\n",
    "\n",
    "    context_span = [(word.idx, word.idx + len(word.text))\n",
    "                    for word in nlp(row.context, disable=['parser', 'tagger', 'ner'])]\n",
    "    starts, ends = zip(*context_span)\n",
    "\n",
    "    answer_start, answer_end = row.label\n",
    "    start_idx = starts.index(answer_start)\n",
    "\n",
    "    end_idx = ends.index(answer_end)\n",
    "\n",
    "    ans_toks = [w.text for w in nlp(row.answer, disable=['parser', 'tagger', 'ner'])]\n",
    "    ans_start = ans_toks[0]\n",
    "    ans_end = ans_toks[-1]\n",
    "    assert idx2word[row.context_ids[start_idx]] == ans_start\n",
    "    assert idx2word[row.context_ids[end_idx]] == ans_end\n",
    "\n",
    "    return [start_idx, end_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "24808b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_to_ids(text, word2idx):\n",
    "    '''\n",
    "    Converts context text to their respective ids by mapping each word\n",
    "    using word2idx. Input text is tokenized using spacy tokenizer first.\n",
    "\n",
    "    :param str text: context text to be converted\n",
    "    :param dict word2idx: word to id mapping\n",
    "\n",
    "    :returns list context_ids: list of mapped ids\n",
    "\n",
    "    :raises assertion error: sanity check\n",
    "\n",
    "    '''\n",
    "\n",
    "    context_tokens = [w.text for w in nlp(text, disable=['parser', 'tagger', 'ner'])]\n",
    "    context_ids = [word2idx[word] for word in context_tokens]\n",
    "\n",
    "    assert len(context_ids) == len(context_tokens)\n",
    "    return context_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "a89fec26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_ids(text, word2idx):\n",
    "    '''\n",
    "    Converts question text to their respective ids by mapping each word\n",
    "    using word2idx. Input text is tokenized using spacy tokenizer first.\n",
    "\n",
    "    :param str text: question text to be converted\n",
    "    :param dict word2idx: word to id mapping\n",
    "    :returns list context_ids: list of mapped ids\n",
    "\n",
    "    :raises assertion error: sanity check\n",
    "\n",
    "    '''\n",
    "\n",
    "    question_tokens = [w.text for w in nlp(text, disable=['parser', 'tagger', 'ner'])]\n",
    "    question_ids = [word2idx[word] for word in question_tokens]\n",
    "\n",
    "    assert len(question_ids) == len(question_tokens)\n",
    "    return question_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "40057cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_indices(df, idx2word):\n",
    "    '''\n",
    "    Performs the tests mentioned above. This method also gets the start and end of the answers\n",
    "    with respect to the context_ids for each example.\n",
    "\n",
    "    :param dataframe df: SQUAD df\n",
    "    :param dict idx2word: inverse mapping of token ids to words\n",
    "    :returns\n",
    "        list start_value_error: example idx where the start idx is not found in the start spans\n",
    "                                of the text\n",
    "        list end_value_error: example idx where the end idx is not found in the end spans\n",
    "                              of the text\n",
    "        list assert_error: examples that fail assertion errors. A majority are due to the above errors\n",
    "\n",
    "    '''\n",
    "\n",
    "    start_value_error = []\n",
    "    end_value_error = []\n",
    "    assert_error = []\n",
    "    for index, row in df.iterrows():\n",
    "\n",
    "        answer_tokens = [w.text for w in nlp(row['answer'], disable=['parser', 'tagger', 'ner'])]\n",
    "\n",
    "        start_token = answer_tokens[0]\n",
    "        end_token = answer_tokens[-1]\n",
    "\n",
    "        context_span = [(word.idx, word.idx + len(word.text))\n",
    "                        for word in nlp(row['context'], disable=['parser', 'tagger', 'ner'])]\n",
    "        if not context_span:\n",
    "            assert_error.append(index)\n",
    "            continue\n",
    "\n",
    "        starts, ends = zip(*context_span)\n",
    "\n",
    "        answer_start, answer_end = row['label']\n",
    "\n",
    "        try:\n",
    "            start_idx = starts.index(answer_start)\n",
    "        except Exception:\n",
    "            start_value_error.append(index)\n",
    "        try:\n",
    "            end_idx = ends.index(answer_end)\n",
    "        except Exception:\n",
    "            end_value_error.append(index)\n",
    "\n",
    "        try:\n",
    "            assert idx2word[row['context_ids'][start_idx]] == answer_tokens[0]\n",
    "            assert idx2word[row['context_ids'][end_idx]] == answer_tokens[-1]\n",
    "        except Exception:\n",
    "            assert_error.append(index)\n",
    "\n",
    "    return start_value_error, end_value_error, assert_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "efb9c5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_indices(df, idx2word):\n",
    "\n",
    "    start_value_error, end_value_error, assert_error = test_indices(df, idx2word)\n",
    "    err_idx = start_value_error + end_value_error + assert_error\n",
    "    err_idx = set(err_idx)\n",
    "    print(f\"Number of error indices: {len(err_idx)}\")\n",
    "\n",
    "    return err_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "babf6b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of error indices: 100\n",
      "Number of error indices: 0\n"
     ]
    }
   ],
   "source": [
    "df['context_ids'] = df.context.apply(context_to_ids, word2idx=word2idx)\n",
    "df['question_ids'] = df.question.apply(question_to_ids, word2idx=word2idx)\n",
    "err = get_error_indices(df, idx2word)\n",
    "df.drop(err, inplace=True)\n",
    "df_label_idx = df.apply(index_answer, axis=1, idx2word=idx2word)\n",
    "df['label_idx'] = df_label_idx\n",
    "l_e=get_error_indices(df, idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "b9509d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id_             WHITESMOKE,INC_11_08_2011-EX-10.26-PROMOTION A...\n",
       "question        Highlight the parts (if any) of this contract ...\n",
       "answer                       Promotion and Distribution Agreement\n",
       "label                                                  [200, 236]\n",
       "context         CH PORTION,  WHICH HAS BEEN OMITTED HEREIN AND...\n",
       "ctx_offset                                                    107\n",
       "context_ids     [2406, 6353, 2, 15, 6354, 590, 423, 997, 2407,...\n",
       "question_ids    [3765, 4, 2631, 6, 132, 55, 5, 8, 19, 401, 310...\n",
       "label_idx                                                [37, 40]\n",
       "Name: 8, dtype: object"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rw_1=df.iloc[3]\n",
    "rw_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b6c8b47d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Promotion', 'and', 'Distribution']"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[idx2word.get(x) for x in rw_1.context_ids[rw_1.label_idx[0]:rw_1.label_idx[1]]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legal-contract-elements",
   "language": "python",
   "name": "legal-contract-elements"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
