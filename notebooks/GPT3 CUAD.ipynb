{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4509da66",
   "metadata": {},
   "source": [
    "# GPT3 CUAD\n",
    "\n",
    "The processing of the CUAD dataset with the GPT3 Q&A engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7786a3d",
   "metadata": {},
   "source": [
    "Read in the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "599146a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0750b2b",
   "metadata": {},
   "source": [
    "**Prepare dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1f1c8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('../data/processed/cuad_drqa/data.pkl')\n",
    "data = data.rename(columns={'id_': 'id'})\n",
    "with open(\"../data/processed/cuad_drqa/drqa_idx2word.pickle\", 'rb') as file:\n",
    "    idx2word = pickle.load(file)\n",
    "with open(\"../data/processed/cuad_drqa/drqa_word2idx.pickle\", 'rb') as file:\n",
    "    word2idx = pickle.load(file)\n",
    "\n",
    "with open(\"../data/processed/cuad_drqa/drqa_word_vocab.pickle\", 'rb') as file:\n",
    "    word_vocab = pickle.load(file)\n",
    "\n",
    "data['todrop'] = False\n",
    "for idx, row in data.iterrows():\n",
    "    t=row\n",
    "    if not [idx2word[t.context_ids[x]] for x in range(t.label_idx[0], t.label_idx[1]+1)]:\n",
    "        row.todrop=True\n",
    "data=data[data['todrop']==False]\n",
    "data.drop(columns=['todrop'],inplace=True)\n",
    "\n",
    "valid_df = data[int(0.7*len(data)):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3bc9dc",
   "metadata": {},
   "source": [
    "**Fetch data from openai**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c050a00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "288it [01:42,  2.80it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "openai.api_key = os.getenv(\"OPENAI_APIKEY\")\n",
    "\n",
    "res_ = {}\n",
    "for idx, row in tqdm(valid_df.iterrows()):\n",
    "    \n",
    "    try:\n",
    "        response = openai.Answer.create(\n",
    "         search_model=\"ada\",\n",
    "         model=\"curie\",\n",
    "         question=row.question,\n",
    "         documents=[row.context],\n",
    "         examples_context=\"In 2017, U.S. life expectancy was 78.6 years.\",\n",
    "         examples=[[\"What is human life expectancy in the United States?\",\"78 years.\"]],\n",
    "         max_tokens=len(row.answer.split(' '))*3,\n",
    "         stop=[\"\\n\", \"<|endoftext|>\"],\n",
    "        )\n",
    "        res_[row.id]=response.to_dict()\n",
    "    except Exception as e:\n",
    "        res_[row.id]={'error':True, 'ags': e.args}    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c828f925",
   "metadata": {},
   "source": [
    "**Prepare for calculation of score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c6c8513",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nt/g2ndh3nx4dj16m34wq3h41lh0000gn/T/ipykernel_63178/968721287.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_df['res']=valid_df.apply(lambda x: res_.get(x.id, {'error':True} ),axis=1)\n"
     ]
    }
   ],
   "source": [
    "valid_df['res']=valid_df.apply(lambda x: res_.get(x.id, {'error':True} ),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00bff2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nt/g2ndh3nx4dj16m34wq3h41lh0000gn/T/ipykernel_63178/2785580575.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_df['res_ans']=valid_df.res.apply(lambda x: x.get('answers',[' '])[0])\n"
     ]
    }
   ],
   "source": [
    "valid_df['res_ans']=valid_df.res.apply(lambda x: x.get('answers',[' '])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ee8cf6",
   "metadata": {},
   "source": [
    "Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84420c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df.to_pickle('./GPT3CUAD_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0be30fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_single(predictions, answers, **kwargs):\n",
    "    '''\n",
    "    Gets a dictionary of predictions with question_id as key\n",
    "    and prediction as value. The validation dataset has multiple \n",
    "    answers for a single question. Hence we compare our prediction\n",
    "    with all the answers and choose the one that gives us\n",
    "    the maximum metric (em or f1). \n",
    "    This method first parses the JSON file, gets all the answers\n",
    "    for a given id and then passes the list of answers and the \n",
    "    predictions to calculate em, f1.\n",
    "\n",
    "\n",
    "    :param dict predictions\n",
    "    Returns\n",
    "    : exact_match: 1 if the prediction and ground truth \n",
    "      match exactly, 0 otherwise.\n",
    "    : f1_score: \n",
    "    '''\n",
    "    assert len(predictions) == len(answers)\n",
    "    f1 = exact_match = total = 0\n",
    "    for key, value in predictions.items():\n",
    "        prediction = value\n",
    "        ground_truths = [answers[key]]\n",
    "\n",
    "        exact_match += metric_max_over_ground_truths(exact_match_score, prediction, ground_truths)\n",
    "        f1 += metric_max_over_ground_truths(f1_score, prediction, ground_truths)\n",
    "\n",
    "    total = len(predictions)\n",
    "    exact_match = 100.0 * exact_match / total\n",
    "    f1 = 100.0 * f1 / total\n",
    "\n",
    "    return exact_match, f1\n",
    "\n",
    "\n",
    "def normalize_answer(s):\n",
    "    '''\n",
    "    Performs a series of cleaning steps on the ground truth and \n",
    "    predicted answer.\n",
    "    '''\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    '''\n",
    "    Returns maximum value of metrics for predicition by model against\n",
    "    multiple ground truths.\n",
    "\n",
    "    :param func metric_fn: can be 'exact_match_score' or 'f1_score'\n",
    "    :param str prediction: predicted answer span by the model\n",
    "    :param list ground_truths: list of ground truths against which\n",
    "                               metrics are calculated. Maximum values of \n",
    "                               metrics are chosen.\n",
    "\n",
    "\n",
    "    '''\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, ground_truth)\n",
    "        scores_for_ground_truths.append(score)\n",
    "\n",
    "    return max(scores_for_ground_truths)\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    '''\n",
    "    Returns f1 score of two strings.\n",
    "    '''\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    '''\n",
    "    Returns exact_match_score of two strings.\n",
    "    '''\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    '''\n",
    "    Helper function to record epoch time.\n",
    "    '''\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71fa8617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def evaluate_single(predictions, answers, **kwargs):\n",
    "    '''\n",
    "    Gets a dictionary of predictions with question_id as key\n",
    "    and prediction as value. The validation dataset has multiple \n",
    "    answers for a single question. Hence we compare our prediction\n",
    "    with all the answers and choose the one that gives us\n",
    "    the maximum metric (em or f1). \n",
    "    This method first parses the JSON file, gets all the answers\n",
    "    for a given id and then passes the list of answers and the \n",
    "    predictions to calculate em, f1.\n",
    "\n",
    "\n",
    "    :param dict predictions\n",
    "    Returns\n",
    "    : exact_match: 1 if the prediction and ground truth \n",
    "      match exactly, 0 otherwise.\n",
    "    : f1_score: \n",
    "    '''\n",
    "    assert len(predictions) == len(answers)\n",
    "    f1 = exact_match = total = 0\n",
    "    for key, value in predictions.items():\n",
    "        prediction = value\n",
    "        ground_truths = [answers[key]]\n",
    "\n",
    "        exact_match += metric_max_over_ground_truths(exact_match_score, prediction, ground_truths)\n",
    "        f1 += metric_max_over_ground_truths(f1_score, prediction, ground_truths)\n",
    "\n",
    "    total = len(predictions)\n",
    "    exact_match = 100.0 * exact_match / total\n",
    "    f1 = 100.0 * f1 / total\n",
    "\n",
    "    return exact_match, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcdfbb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions={row.id:row.res_ans for _, row in valid_df.iterrows()}\n",
    "answers= {row.id:row.answer for _, row in valid_df.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9955b9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "em, f1 = evaluate_single(predictions,answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f757079c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.930555555555555"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80a9ab95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.37516937973732"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legal-contract-elements",
   "language": "python",
   "name": "legal-contract-elements"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
