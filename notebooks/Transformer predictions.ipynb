{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93d34a21",
   "metadata": {},
   "source": [
    "# Transformer model testing\n",
    "\n",
    "Very basic notebooks for testing transformer model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "43636ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from src.models.drqa.drqa_train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6b2d65f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "# executing these commands for the first time initiates a download of the \n",
    "# model weights to ~/.cache/torch/transformers/\n",
    "MDL=\"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MDL)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(MDL, return_dict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa9b49b",
   "metadata": {},
   "source": [
    "# Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f8e23830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "334094338"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed042e05",
   "metadata": {},
   "source": [
    "# Out of the box performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "17d631f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f16c563e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "960it [39:48,  2.49s/it]\n"
     ]
    }
   ],
   "source": [
    "df= pd.read_pickle('./data/processed/cuad_drqa/data.pkl')\n",
    "predictions = {}\n",
    "answers = {}\n",
    "for idx, row in tqdm(df.iterrows()):\n",
    "    question=row.question\n",
    "    # limit context - Do intelligently. Ensure answer is within span.\n",
    "    context = row.context[0:512]\n",
    "\n",
    "    # 1. TOKENIZE THE INPUT\n",
    "    inputs = tokenizer.encode_plus(question, context, return_tensors=\"pt\") \n",
    "\n",
    "    # 2. OBTAIN MODEL SCORES\n",
    "    # the AutoModelForQuestionAnswering class includes a span predictor on top of the model. \n",
    "    # the model returns answer start and end scores for each word in the text\n",
    "    answer_start_scores, answer_end_scores = model(**inputs)\n",
    "    answer_start = torch.argmax(answer_start_scores)  \n",
    "    answer_end = torch.argmax(answer_end_scores) + 1  \n",
    "\n",
    "    # 3. GET THE ANSWER SPAN\n",
    "    ans = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end]))\n",
    "    predictions[row.id_]=ans\n",
    "    answers[row.id_]=row.answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc1d6f0",
   "metadata": {},
   "source": [
    "Evaluate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "974146a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_single(predictions, answers, **kwargs):\n",
    "    '''\n",
    "    Gets a dictionary of predictions with question_id as key\n",
    "    and prediction as value. The validation dataset has multiple \n",
    "    answers for a single question. Hence we compare our prediction\n",
    "    with all the answers and choose the one that gives us\n",
    "    the maximum metric (em or f1). \n",
    "    This method first parses the JSON file, gets all the answers\n",
    "    for a given id and then passes the list of answers and the \n",
    "    predictions to calculate em, f1.\n",
    "\n",
    "\n",
    "    :param dict predictions\n",
    "    Returns\n",
    "    : exact_match: 1 if the prediction and ground truth \n",
    "      match exactly, 0 otherwise.\n",
    "    : f1_score: \n",
    "    '''\n",
    "    assert len(predictions) == len(answers)\n",
    "    f1 = exact_match = total = 0\n",
    "    for key, value in predictions.items():\n",
    "        prediction = value\n",
    "        ground_truths = [answers[key]]\n",
    "\n",
    "        exact_match += metric_max_over_ground_truths(exact_match_score, prediction, ground_truths)\n",
    "        f1 += metric_max_over_ground_truths(f1_score, prediction, ground_truths)\n",
    "\n",
    "    total = len(predictions)\n",
    "    exact_match = 100.0 * exact_match / total\n",
    "    f1 = 100.0 * f1 / total\n",
    "\n",
    "    return exact_match, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a0000330",
   "metadata": {},
   "outputs": [],
   "source": [
    "em, f1 = evaluate_single(predictions,answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "16dad640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model bert-large-uncased-whole-word-masking-finetuned-squad has an \n",
      " exact match score of: 2.9166666666666665\n",
      " f1 of: 16.310703236473493\n"
     ]
    }
   ],
   "source": [
    "print(f\"The model {MDL} has an \\n exact match score of: {em}\\n f1 of: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legal-contract-elements",
   "language": "python",
   "name": "legal-contract-elements"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
