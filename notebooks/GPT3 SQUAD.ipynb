{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "763e3451",
   "metadata": {},
   "source": [
    "# GPT3 CUAD\n",
    "\n",
    "The processing of the CUAD dataset with the GPT3 Q&A engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b3ca869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.models.drqa.drqa_dataset import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969854eb",
   "metadata": {},
   "source": [
    "**Get and prepare data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7ee6f51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5903\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator()\n",
    "valid_dataset = DRQADataset(\"../data/processed/squad_drqa/drqa_valid.pkl\")\n",
    "valid_dataset.frame = valid_dataset.frame.drop_duplicates(subset=['id'])\n",
    "print(len(valid_dataset))\n",
    "validloader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=10,\n",
    "    shuffle=True,\n",
    "    generator=g,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08b2de2",
   "metadata": {},
   "source": [
    "**Fetch data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c050a00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5903it [33:04,  2.98it/s]\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from tqdm import tqdm\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_APIKEY')\n",
    "\n",
    "valid_dataset.frame['res']=''\n",
    "for idx, row in tqdm(valid_dataset.frame.iterrows()):\n",
    "    \n",
    "    try:\n",
    "        response = openai.Answer.create(\n",
    "         search_model=\"ada\",\n",
    "         model=\"curie\",\n",
    "         question=row.question,\n",
    "         documents=[row.context],\n",
    "         examples_context=\"In 2017, U.S. life expectancy was 78.6 years.\",\n",
    "         examples=[[\"What is human life expectancy in the United States?\",\"78 years.\"]],\n",
    "         max_tokens=len(row.answer.split(' '))*3,\n",
    "         stop=[\"\\n\", \"<|endoftext|>\"],\n",
    "        )\n",
    "        row.res=response.to_dict()\n",
    "    except Exception as e:\n",
    "        row.res={'error':True, 'ags': e.args}    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f443e9c2",
   "metadata": {},
   "source": [
    "Process response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "00bff2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset.frame['res_ans']=valid_dataset.frame.res.apply(lambda x: x.get('answers',[' '])[0].replace('.',''))\n",
    "valid_dataset.frame['res_ans']=valid_dataset.frame.res.apply(lambda x: x.get('answers',[' '])[0].replace('.','').lower().replace('the ',''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6465f3",
   "metadata": {},
   "source": [
    "Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "84420c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset.frame.to_pickle('./GPT3SQuAD_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97da1797",
   "metadata": {},
   "source": [
    "Calculate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "429ebf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "def evaluate(predictions, **kwargs):\n",
    "    '''\n",
    "    Gets a dictionary of predictions with question_id as key\n",
    "    and prediction as value. The validation dataset has multiple \n",
    "    answers for a single question. Hence we compare our prediction\n",
    "    with all the answers and choose the one that gives us\n",
    "    the maximum metric (em or f1). \n",
    "    This method first parses the JSON file, gets all the answers\n",
    "    for a given id and then passes the list of answers and the \n",
    "    predictions to calculate em, f1.\n",
    "\n",
    "\n",
    "    :param dict predictions\n",
    "    Returns\n",
    "    : exact_match: 1 if the prediction and ground truth \n",
    "      match exactly, 0 otherwise.\n",
    "    : f1_score: \n",
    "    '''\n",
    "\n",
    "    # TODO: Change to correct directory\n",
    "    with open('../data/raw/dev-v1.1.json', 'r', encoding='utf-8') as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    dataset = dataset['data']\n",
    "    f1 = exact_match = total = 0\n",
    "    for article in dataset:\n",
    "        for paragraph in article['paragraphs']:\n",
    "            for qa in paragraph['qas']:\n",
    "                total += 1\n",
    "                if qa['id'] not in predictions:\n",
    "                    continue\n",
    "\n",
    "                ground_truths = list(map(lambda x: x['text'], qa['answers']))\n",
    "                \n",
    "                prediction = predictions[qa['id']]\n",
    "\n",
    "                exact_match += metric_max_over_ground_truths(\n",
    "                    exact_match_score, prediction, ground_truths)\n",
    "\n",
    "                f1 += metric_max_over_ground_truths(\n",
    "                    f1_score, prediction, ground_truths)\n",
    "\n",
    "    exact_match = 100.0 * exact_match / total\n",
    "    f1 = 100.0 * f1 / total\n",
    "\n",
    "    return exact_match, f1\n",
    "\n",
    "\n",
    "def evaluate_single(predictions, answers, **kwargs):\n",
    "    '''\n",
    "    Gets a dictionary of predictions with question_id as key\n",
    "    and prediction as value. The validation dataset has multiple \n",
    "    answers for a single question. Hence we compare our prediction\n",
    "    with all the answers and choose the one that gives us\n",
    "    the maximum metric (em or f1). \n",
    "    This method first parses the JSON file, gets all the answers\n",
    "    for a given id and then passes the list of answers and the \n",
    "    predictions to calculate em, f1.\n",
    "\n",
    "\n",
    "    :param dict predictions\n",
    "    Returns\n",
    "    : exact_match: 1 if the prediction and ground truth \n",
    "      match exactly, 0 otherwise.\n",
    "    : f1_score: \n",
    "    '''\n",
    "    assert len(predictions) == len(answers)\n",
    "    f1 = exact_match = total = 0\n",
    "    for key, value in predictions.items():\n",
    "        prediction = value\n",
    "        ground_truths = [answers[key]]\n",
    "\n",
    "        exact_match += metric_max_over_ground_truths(exact_match_score, prediction, ground_truths)\n",
    "        f1 += metric_max_over_ground_truths(f1_score, prediction, ground_truths)\n",
    "\n",
    "    total = len(predictions)\n",
    "    exact_match = 100.0 * exact_match / total\n",
    "    f1 = 100.0 * f1 / total\n",
    "\n",
    "    return exact_match, f1\n",
    "\n",
    "\n",
    "def normalize_answer(s):\n",
    "    '''\n",
    "    Performs a series of cleaning steps on the ground truth and \n",
    "    predicted answer.\n",
    "    '''\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    '''\n",
    "    Returns maximum value of metrics for predicition by model against\n",
    "    multiple ground truths.\n",
    "\n",
    "    :param func metric_fn: can be 'exact_match_score' or 'f1_score'\n",
    "    :param str prediction: predicted answer span by the model\n",
    "    :param list ground_truths: list of ground truths against which\n",
    "                               metrics are calculated. Maximum values of \n",
    "                               metrics are chosen.\n",
    "\n",
    "\n",
    "    '''\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, ground_truth)\n",
    "        scores_for_ground_truths.append(score)\n",
    "\n",
    "    return max(scores_for_ground_truths)\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    '''\n",
    "    Returns f1 score of two strings.\n",
    "    '''\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    '''\n",
    "    Returns exact_match_score of two strings.\n",
    "    '''\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    '''\n",
    "    Helper function to record epoch time.\n",
    "    '''\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "71fa8617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25.10879848628193, 34.42828376790898)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate({x.id: x.res_ans for idx, x in valid_dataset.frame.iterrows()})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legal-contract-elements",
   "language": "python",
   "name": "legal-contract-elements"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
